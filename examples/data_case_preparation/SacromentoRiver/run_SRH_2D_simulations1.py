"""
Perform Monte Carlo simulation with the SRH_2D_Model and SRH_2D_Data classes using serial, parallel (multiprocessing or mpi). If using multiprocessing, it can only run in one machine. If using mpi, it can run in multiple machines.

# For serial execution
python run_SRH_2D_simulations.py

# For multiprocessing
python run_SRH_2D_simulations.py

# For MPI execution
mpirun -np <number_of_processes> python run_SRH_2D_simulations.py

It uses pyHMT2D to control the run of SRH-2D model, extract information, and parallelization.

The parameter values for the Monte Carlo simulation are read in from a file, which should be generated by the preprocessing script.
"""

#if run in the cloud, need to specify the location of pyHMT2D. If pyHMT2D is installed
#with regular pip install, then the following is not necessary.
#import sys
#sys.path.append(r"C:\Users\Administrator\python_packages\pyHMT2D")

import pyHMT2D

import numpy as np

import matplotlib.pyplot as plt
from matplotlib.ticker import StrMethodFormatter

import multiprocessing
import os
import sys
import json
import platform
import time
from datetime import datetime
from mpi4py import MPI  # Add MPI import

# Set the random seed before generating any random numbers
np.random.seed(123456)  # You can use any integer as the seed

# Add the SRH_2D_automation directory to Python path (adjust this path as needed)
root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../"))
sys.path.append(root_dir)

# Now import the SRH_2D_solver_module 
from SRH_2D_solver_module import run_one_SRH_2D_case_with_multiple_inlet_q

plt.rc('text', usetex=False)  #allow the use of Latex for math expressions and equations
plt.rc('font', family='serif') #specify the default font family to be "serif"


def Monte_Carlo_simulations_multiprocessing(srhcontrol_file, nSamples, Q_bc_IDs, inlet_Q_samples, system_name, nProcess, bDeleteCaseDir=True):
    """
    Run Monte Carlo simulations nSamples times. Run all cases in parallel using multiprocessing (only works on one machine). Each core
    simulates one case at a time.

    In this example, only the inflow is varied.

    Parameters
    ----------
    srhcontrol_file : str
        Name of the SRH-2D control file (either srhhydro or SIF file), e.g., "case_SIF.dat" or "case.srhhydro"
    nSamples : int
        number of samples
    Q_bc_IDs : list of int
        IDs of the inlet boundary conditions
    inlet_Q_samples : numpy array
        list of inlet Q values
    system_name : str
        Name of the operating system, e.g., "Windows" or "Linux"
    nProcess : int
        number of parallel processes to use
    bDeleteCaseDir : bool
        whether to delete case directory when simulation is done. Default is True. If it is set to False, the case directory
        will be kept. However, be cautious on this. If the number of simulations is large, all cases together will use
        very large space on the hard disk.

        In this example, the simulation result is saved into a VTK file (thus the case directory is not needed after
        simulation is done).

    Returns
    -------

    """

    #print("Running Monte Carlo simulations in parallel with", nProcess, "processes ...")

    #check the number of parameters in Q_bc_IDs and inlet_Q_samples
    if len(Q_bc_IDs) != inlet_Q_samples.shape[1]:
        raise ValueError("The number of parameters in Q_bc_IDs and inlet_Q_samples must be the same.")

    #create a pool of parallel workers
    pool = multiprocessing.Pool(processes=nProcess)

    #make the directory "cases". If it already exists, reminder user that the contents within may be overwritten.
    if os.path.isdir("cases"):
        print("The directory cases already exists. The contents within may be overwritten.")
        #quit()
    else:
        os.mkdir("cases")

    #create the list of case IDs to be assigned to the parallel workers (case ID is 1-based)
    caseIDs = [*range(1, nSamples+1, 1)]    

    #prepare the arguments for calling the "run_one_SRH_2D_case(...)" function
    #args: case_ID, srhcontrol_file, ManningN_MaterialID, ManningN, ManningN_MaterialName, Q_bc_ID, Q,  WSE_bc_ID, WSE, system_name
    #args: case_ID, srhcontrol_file, Q_bc_IDs, Qs, system_name, bDeleteCaseDir=True
    args = [(caseID, srhcontrol_file, Q_bc_IDs, inlet_Q_samples[caseID-1,:], system_name, bDeleteCaseDir) for caseID in caseIDs]
    
    #optional or for debugging: only run a subset of cases 
    args = args[:5]  #only run the first 10 cases 
    #args = args[0:200]  #only run the first 1000 cases
    #args = args[1000:2000]  #only run the first cases 1000 to 1999
    #args = args[50, 51, 108, 99]  #only run the first cases in the list

    #print("args = ", args)

    # do the work
    outputs = pool.starmap(run_one_SRH_2D_case_with_multiple_inlet_q, args)    

    print("Parallel run of cases done.")

    # Count the number of successful and failed runs
    successful_runs = [output for output in outputs if output > 0]
    failed_runs = [output for output in outputs if output < 0]
    print("    Number of successful runs = ", len(successful_runs))
    print("    Number of failed runs = ", len(failed_runs))

    # save the returned outputs as a json file: outputs contains the caseIDs (positive if the case run was successful,
    # negative otherwise); also save successful and failed runs in the same json file
    # prepare the data to be saved
    date_time = datetime.now().strftime("%Y_%m_%d-%I_%M_%S_%p")    #take the current date and time
    output_filename = f'parallel_run_SRH_2D_outputs_{date_time}.json'
    data_to_save = {
        "nSamples": nSamples,
        "nSamples_successful": len(successful_runs),
        "nSamples_failed": len(failed_runs),
        "all_run_results": outputs,
        "successful_runs": successful_runs,
        "failed_runs": failed_runs
    }
    with open(output_filename, 'w') as f:
        json.dump(data_to_save, f, indent=4)

def Monte_Carlo_simulations_serial(srhcontrol_file, nSamples, Q_bc_IDs, inlet_Q_samples, system_name, nProcess, bDeleteCaseDir=True):
    """
    Run Monte Carlo simulations in serial

    Parameters
    ----------
    srhcontrol_file : str
        Name of the SRH-2D control file (either srhhydro or SIF file), e.g., "case_SIF.dat" or "case.srhhydro"
    nSamples : int
        number of samples
    Q_bc_IDs : list of int
        IDs of the inlet boundary conditions
    inlet_Q_samples : numpy array
        list of inlet Q values
    system_name : str
        Name of the operating system, e.g., "Windows" or "Linux"
    nProcess : int
        number of parallel processes to use
    bDeleteCaseDir : bool
        whether to delete case directory when simulation is done. Default is True. If it is set to False, the case directory
        will be kept. However, be cautious on this. If the number of simulations is large, all cases together will use
        very large space on the hard disk.

        In this example, the SRH-2D result is saved into a VTK file (thus the case directory is not needed after
        simulation is done).

    Returns
    -------

    """

    print("Running Monte Carlo simulations in serial ...")

    #check the number of parameters in Q_bc_IDs and inlet_Q_samples
    if len(Q_bc_IDs) != inlet_Q_samples.shape[1]:
        raise ValueError("The number of parameters in Q_bc_IDs and inlet_Q_samples must be the same.")

    
    #access the parameter values
    Manning_n_samples = samples['Manning_n']
    Q_samples = samples['Q']
    WSE_samples = samples['WSE']

    #make the directory "cases". If it already exists, reminder user that the contents within may be overwritten.
    if os.path.isdir("cases"):
        print("The directory cases already exists. The contents within may be overwritten.")
        #quit()
    else:
        os.mkdir("cases")

    #list to collect outputs
    outputs = []

    #for caseID in range(1, nSamples+1):   #run all cases in the sample file (case ID starts from 1)
    for caseID in range(1, 3):  #For debugging/testing, only run the first 3 cases
        print("    Running Monte Carlo simulation # ", caseID, "out of", nSamples)

        output = run_one_SRH_2D_case_with_multiple_inlet_q(caseID, srhcontrol_file, Q_bc_IDs, inlet_Q_samples[caseID-1,:], system_name, bDeleteCaseDir)

        outputs.append(output)
    
    print("    Serial run of cases done.")    

    # Count the number of successful and failed runs
    successful_runs = [output for output in outputs if output > 0]
    failed_runs = [output for output in outputs if output < 0]
    print("    Number of successful runs = ", len(successful_runs))
    print("    Number of failed runs = ", len(failed_runs))

    # save the returned outputs as a json file: outputs contains the caseIDs (positive if the case run was successful,
    # negative otherwise); also save successful and failed runs in the same json file
    # prepare the data to be saved
    date_time = datetime.now().strftime("%Y_%m_%d-%I_%M_%S_%p")    #take the current date and time
    output_filename = f'serial_run_SRH_2D_outputs_{date_time}.json'
    data_to_save = {
        "nSamples": nSamples,
        "nSamples_successful": len(successful_runs),
        "nSamples_failed": len(failed_runs),
        "all_run_results": outputs,
        "successful_runs": successful_runs,
        "failed_runs": failed_runs
    }
    with open(output_filename, 'w') as f:
        json.dump(data_to_save, f, indent=4)

def Monte_Carlo_simulations_mpi(srhcontrol_file, nSamples, Q_bc_IDs, inlet_Q_samples, system_name, bDeleteCaseDir=True):
    """
    Run Monte Carlo simulations using MPI for distributed computing across multiple nodes.
    """
    # Get MPI information
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()
    
    # Only rank 0 creates the cases directory (if it does not exist)
    if rank == 0:
        if os.path.isdir("cases"):
            print("The directory cases already exists. The contents within may be overwritten.")
        else:
            os.mkdir("cases")
    
    # Synchronize all processes
    comm.Barrier()

    # Distribute work among processes
    # Allow specifying a range of case IDs to run
    start_case_id = 1  # start case ID
    end_case_id = 250 #nSamples  # end case ID
    #end_case_id = 8  # end case ID   #debug
        
    # Adjust the number of samples to the specified range
    nSamples_in_range = end_case_id - start_case_id + 1
    
    # Distribute the work among processes
    cases_per_proc = nSamples_in_range // size
    remainder = nSamples_in_range % size
    
    if rank < remainder:
        start_case = start_case_id + rank * (cases_per_proc + 1)
        end_case = start_case + cases_per_proc
    else:
        start_case = start_case_id + rank * cases_per_proc + remainder
        end_case = start_case + cases_per_proc - 1
    
    if rank == 0:
        print(f"Running cases from {start_case_id} to {end_case_id} (total: {nSamples_in_range} cases)")
        print(f"Work distribution: {size} processes, {cases_per_proc} cases per process, {remainder} processes get 1 extra case")
    
    # Run local cases
    local_outputs = []
    for caseID in range(start_case, end_case + 1):
        print(f"Process {rank} running case {caseID}")
        output = run_one_SRH_2D_case_with_multiple_inlet_q(
            caseID, srhcontrol_file, Q_bc_IDs, 
            inlet_Q_samples[caseID-1,:], system_name, bDeleteCaseDir
        )
        local_outputs.append(output)
    
    # Gather results from all processes
    all_outputs = comm.gather(local_outputs, root=0)
    
    # Process 0 handles the results
    if rank == 0:
        outputs = [item for sublist in all_outputs for item in sublist]
        successful_runs = [output for output in outputs if output > 0]
        failed_runs = [output for output in outputs if output < 0]
        
        print(f"Number of successful runs = {len(successful_runs)}")
        print(f"Number of failed runs = {len(failed_runs)}")
        
        # Save results
        date_time = datetime.now().strftime("%Y_%m_%d-%I_%M_%S_%p")
        output_filename = f'mpi_run_SRH_2D_outputs_{date_time}.json'
        data_to_save = {
            "nSamples": nSamples,
            "nSamples_successful": len(successful_runs),
            "nSamples_failed": len(failed_runs),
            "all_run_results": outputs,
            "successful_runs": successful_runs,
            "failed_runs": failed_runs
        }
        with open(output_filename, 'w') as f:
            json.dump(data_to_save, f, indent=4)

if __name__ == "__main__":
    # Read configuration first
    print("Reading the configuration file ...")
    with open('simulations_config.json', 'r') as f:
        config = json.load(f)

    # Initialize MPI if needed
    if 'execution_mode' in config['run_specs'] and config['run_specs']['execution_mode'] == 'mpi':
        comm = MPI.COMM_WORLD
        rank = comm.Get_rank()
    else:
        rank = 0

    # Only rank 0 prints initial messages and reads parameters
    if rank == 0:
        start_time = time.time()
        parameters_specs = config['parameter_specs']
        nSamples = parameters_specs['n_samples']
        
        run_specs = config['run_specs']
        case_name = run_specs['case_name']
        sampled_parameters_file = run_specs['sampled_parameters_file']

        # Read the sampled parameters
        inlet_Q_samples = np.loadtxt(sampled_parameters_file, skiprows=1)
        print("inlet_Q_samples.shape = ", inlet_Q_samples.shape)
        print("some inlet Q samples = ", inlet_Q_samples[:5,:])

        # Q_bc_IDs
        Q_bc_IDs = run_specs['Q_bc_IDs']
        
        # Get the system name
        system_name = platform.system()

        # The control file name for the base case    
        if system_name == "Windows":
            srhcontrol_file = case_name+".srhhydro"
        elif system_name == "Linux":
            srhcontrol_file = case_name+"_SIF.dat"
        else:
            raise ValueError("Unsupported operating system: " + system_name)

    # Broadcast configuration and parameters if using MPI
    if 'execution_mode' in config['run_specs'] and config['run_specs']['execution_mode'] == 'mpi':
        # Broadcast the configuration
        config = comm.bcast(config if rank == 0 else None, root=0)
        # Broadcast other necessary variables
        nSamples = comm.bcast(nSamples if rank == 0 else None, root=0)
        inlet_Q_samples = comm.bcast(inlet_Q_samples if rank == 0 else None, root=0)
        Q_bc_IDs = comm.bcast(Q_bc_IDs if rank == 0 else None, root=0)
        srhcontrol_file = comm.bcast(srhcontrol_file if rank == 0 else None, root=0)
        system_name = comm.bcast(system_name if rank == 0 else None, root=0)

    # Get execution mode and parameters
    execution_mode = config['run_specs'].get('execution_mode', 'serial')
    nProcess = config['run_specs'].get('nProcess', 1)
    bDeleteCaseDir = config['run_specs'].get('bDeleteCaseDir', True)

    # Print execution mode information
    if rank == 0:
        if execution_mode == 'mpi':
            print(f"Running Monte Carlo simulations using MPI with {comm.Get_size()} processes")
        elif execution_mode == 'multiprocessing':
            print(f"Running Monte Carlo simulations using multiprocessing with {nProcess} processes")
        else:
            print("Running Monte Carlo simulations in serial mode")

    # Run simulations based on execution mode
    if execution_mode == 'mpi':
        Monte_Carlo_simulations_mpi(srhcontrol_file, nSamples, Q_bc_IDs, 
                                  inlet_Q_samples, system_name, bDeleteCaseDir)
    elif execution_mode == 'multiprocessing':
        Monte_Carlo_simulations_multiprocessing(srhcontrol_file, nSamples, Q_bc_IDs, 
                                      inlet_Q_samples, system_name, nProcess, 
                                      bDeleteCaseDir)
    else:
        Monte_Carlo_simulations_serial(srhcontrol_file, nSamples, Q_bc_IDs, 
                                    inlet_Q_samples, system_name, bDeleteCaseDir)

    # Only rank 0 prints final timing
    if rank == 0:
        end_time = time.time()
        print("Total time taken: ", (end_time - start_time)/3600, " hours")
        print("All done!")


